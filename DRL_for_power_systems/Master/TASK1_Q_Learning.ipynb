{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Obinrzx2wXBA"
   },
   "source": [
    "# Task 1) Tabular Q-Learning with Taxi-v3 \n",
    "In this task, you'll implement **Tabular Q-Learning** algorithm. You will use it for the Taxi-v3 Gym environment to learn to **navigate in a city to transport its passengers from point A to point B.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuZAjCnEwaP4"
   },
   "source": [
    "### Step 0: Install and import the libraries\n",
    "We will use different libraries:\n",
    "- `Numpy`: For handling our Q-table.\n",
    "- `OpenAI Gym`: Contains our ðŸš• environment (Taxi-v3).\n",
    "- `Random`: To generate random numbers (that will be useful for Epsilon-Greedy Policy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hU5lxL3CxFIg"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20tSdDbxxK_H"
   },
   "source": [
    "## Step 1: Create the environment \n",
    "- Here we'll create the Taxi-v3 environment. \n",
    "\n",
    "Our environment looks like this: \n",
    "- It's a **5x5 grid world**\n",
    "- Our ðŸš• is spawned **randomly** in a square. \n",
    "- The passenger is **spawned randomly in one of the 4 possible locations** (R, B, G, Y) and wishes to go in one of the **4 possibles locations too**.\n",
    "\n",
    "The reward system:\n",
    "- -1 for each timestep\n",
    "- +20 for successfully deliver the passenger\n",
    "- -10 for illegal actions (pickup or putdown the passenger at the outside of the destination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "id": "oXUVzEu8xRiF",
    "outputId": "c357a9af-26e4-40ad-b095-4dce8d7c63e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : |\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\")\n",
    "env.reset()\n",
    "print(env.render(mode='ansi'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3xll7Bmxemc"
   },
   "source": [
    "## Step 2: Create the Q-table and initialize it\n",
    "- Now, we'll create our Q-table, to know how much rows (states) and columns (actions) we need to know the **action_space and the observation_space**.\n",
    "- OpenAI Gym provides us a way to do that: `env.action_space.n` and `env.observation_space.n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "aP9i7HRUxKaO",
    "outputId": "d017d2a7-9478-40bc-c405-3480815dc061"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  500  possible states\n",
      "There are  6  possible actions\n"
     ]
    }
   ],
   "source": [
    "state_space = env.observation_space.n\n",
    "print(\"There are \", state_space, \" possible states\")\n",
    "action_space = env.action_space.n\n",
    "print(\"There are \", action_space, \" possible actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "id": "RK00re1ux8He",
    "outputId": "b2729e73-88e1-4463-a725-e28ddecbf0b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "(500, 6)\n"
     ]
    }
   ],
   "source": [
    "# Create our Q table with state_size rows and action_size columns (500x6)\n",
    "Q = np.zeros((state_space, action_space))\n",
    "print(Q)\n",
    "print(Q.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "no8VizkczcY1"
   },
   "source": [
    "## Step 3: Define the hyperparameters\n",
    "- Here, we'll specify the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4fCIcSZhzes3"
   },
   "outputs": [],
   "source": [
    "total_episodes = 25000        # Total number of training episodes\n",
    "total_test_episodes = 100     # Total number of test episodes\n",
    "max_steps = 200               # Max steps per episode\n",
    "\n",
    "learning_rate = 0.01          # Learning rate\n",
    "gamma = 0.99                  # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.001            # Minimum exploration probability \n",
    "decay_rate = 0.01             # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4T-oXbikx6jU"
   },
   "source": [
    "## Step 4: Define the epsilon-greedy policy\n",
    "\n",
    "Epsilon Greedy:\n",
    "\n",
    "- *With probability 1â€Š-â€ŠÉ›*Â : we do **exploitation** (aka our agent selects the action with the highest state-action pair value).\n",
    "\n",
    "- *With probability É›*: we do **exploration** (trying random action).\n",
    "\n",
    "And as the training goes, **we progressively reduce the epsilon value** since we will **need less and less exploration and more exploitation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iWw9brgtx6DS"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Q, state):\n",
    "  # if random number > greater than epsilon --> exploitation\n",
    "  if(random.uniform(0,1) > epsilon):\n",
    "    action = np.argmax(Q[state])\n",
    "  # else --> exploration\n",
    "  else:\n",
    "    action = env.action_space.sample()\n",
    "  \n",
    "  return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qFTQYzwrIet"
   },
   "source": [
    "## Step 5: Define the Q-Learning algorithm and train our agent\n",
    "- Now we implement the Q learning algorithm and the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWYibdun-uDO"
   },
   "outputs": [],
   "source": [
    " for episode in tqdm(range(total_episodes)):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        #\n",
    "        action = epsilon_greedy_policy(Q, state)\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        Q[state][action] = Q[state][action] + learning_rate * (reward + gamma * \n",
    "                                    np.max(Q[new_state]) - Q[state][action])      \n",
    "        # If done : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1U8Qymsitpw4"
   },
   "source": [
    "## Step 6: Testing and Visualization\n",
    "- By running the following cells, you'll see your smart taxi agent.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        print(f\"Episode: {frame['episode']}\")\n",
    "        sleep(.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WlJYOh0yBHZO",
    "outputId": "2199002b-8550-4d4e-bb1b-911e700c90bb"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "rewards = []\n",
    "\n",
    "frames = []\n",
    "for episode in tqdm(range(total_test_episodes)):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "    for step in range(max_steps):\n",
    "        env.render()     \n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(Q[state][:])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        total_rewards += reward\n",
    "        # Put each rendered frame into dict for animation\n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'episode': episode + 1\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "        if done:\n",
    "            rewards.append(total_rewards)\n",
    "            #print (\"Score\", total_rewards)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()\n",
    "print_frames(frames)\n",
    "print (\"Average episodic score: \" +  str(sum(rewards)/total_test_episodes))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Q-Learning with Taxi-v3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
